{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:07:42.537989Z",
     "start_time": "2025-12-10T05:07:42.532130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "import analysis_utils\n",
    "import importlib\n",
    "importlib.reload(analysis_utils)\n",
    "\n",
    "LOOKBACK_DAYS = 30 \n",
    "TARGET_COLUMN = \"Rate_Change\"  \n"
   ],
   "id": "88d3cb24231da376",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:07:46.425113Z",
     "start_time": "2025-12-10T05:07:46.421062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def build_global_indices(speeches, topic_scores, rates_df):\n",
    "    \n",
    "    # 1) Authors\n",
    "    author_names = sorted({v[\"author\"] for v in speeches.values()})\n",
    "    author2idx = {name: i for i, name in enumerate(author_names)}\n",
    "\n",
    "    # 2) Topics\n",
    "    topic_names = set()\n",
    "    for sid, topics in topic_scores.items():\n",
    "        for tname in topics.keys():\n",
    "            topic_names.add(tname)\n",
    "    topic_names = sorted(topic_names)\n",
    "    topic2idx = {name: i for i, name in enumerate(topic_names)}\n",
    "\n",
    "    # 3) Speech ids\n",
    "    speech_ids = sorted(speeches.keys())\n",
    "    speech2idx = {sid: i for i, sid in enumerate(speech_ids)}\n",
    "\n",
    "    # 4) Dates\n",
    "    all_dates = sorted(set(rates_df.index))  # dates where we have rates\n",
    "    date2idx = {d: i for i, d in enumerate(all_dates)}\n",
    "\n",
    "    return {\n",
    "        \"author2idx\": author2idx,\n",
    "        \"topic2idx\": topic2idx,\n",
    "        \"speech2idx\": speech2idx,\n",
    "        \"date2idx\": date2idx,\n",
    "        \"dates\": all_dates,\n",
    "    }"
   ],
   "id": "64256c5ca1c45a42",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:07:49.558009Z",
     "start_time": "2025-12-10T05:07:49.554995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def get_speeches_in_window(target_date, lookback_days, speeches_by_date):\n",
    "    \"\"\"\n",
    "    Returns a list of speech IDs whose date is in [target_date - lookback_days + 1, target_date].\n",
    "    \"\"\"\n",
    "    start_date = target_date - timedelta(days=lookback_days - 1)\n",
    "    cur_date = start_date\n",
    "    selected = []\n",
    "\n",
    "    while cur_date <= target_date:\n",
    "        if cur_date in speeches_by_date:\n",
    "            selected.extend(speeches_by_date[cur_date])\n",
    "        cur_date += timedelta(days=1)\n",
    "\n",
    "    return selected\n",
    "\n"
   ],
   "id": "1b076c17abfb1d6c",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:07:53.695586Z",
     "start_time": "2025-12-10T05:07:53.691138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def build_all_graphs(\n",
    "    speeches,\n",
    "    speeches_with_embeddings,\n",
    "    topic_scores,\n",
    "    rates_df,\n",
    "    out_dir=\"graphs\",\n",
    "    lookback_days=LOOKBACK_DAYS,\n",
    "    target_column=TARGET_COLUMN\n",
    "):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    global_idx = build_global_indices(speeches, topic_scores, rates_df)\n",
    "    speeches_by_date = analysis_utils.group_speeches_by_date(speeches)\n",
    "    print(len(speeches), len(speeches_by_date))\n",
    "    graphs = []\n",
    "    dates = global_idx[\"dates\"]\n",
    "    print(len(dates))\n",
    "    for d in dates:\n",
    "        g = build_graph_for_date(\n",
    "            d,\n",
    "            speeches,\n",
    "            speeches_with_embeddings,\n",
    "            topic_scores,\n",
    "            rates_df,\n",
    "            speeches_by_date,\n",
    "            global_idx,\n",
    "            lookback_days=lookback_days,\n",
    "            target_column=target_column,\n",
    "        )\n",
    "        if g is None:\n",
    "            continue\n",
    "        graphs.append(g)\n",
    "\n",
    "    for i, g in enumerate(graphs):\n",
    "        torch.save(g, out_dir / f\"graph_{i:04d}.pt\")\n",
    "\n",
    "    print(f\"Built {len(graphs)} graphs and saved to {out_dir}\")\n",
    "    return graphs\n"
   ],
   "id": "452330154a4c6ba0",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:53:03.479695Z",
     "start_time": "2025-12-10T05:53:03.469486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build_graphs.py (part 5)\n",
    "\n",
    "def build_graph_for_date(\n",
    "    d,\n",
    "    speeches,\n",
    "    speeches_with_embeddings,\n",
    "    topic_scores,\n",
    "    rates_df,\n",
    "    speeches_by_date,\n",
    "    global_idx,\n",
    "    lookback_days=LOOKBACK_DAYS,\n",
    "    target_column=TARGET_COLUMN\n",
    "):\n",
    "    \"\"\"\n",
    "    Build HeteroData graph snapshot for date d.\n",
    "    Speech nodes store embeddings.\n",
    "    Topic-speech edges store topic score features.\n",
    "    \"\"\"\n",
    "\n",
    "    date2idx = global_idx[\"date2idx\"]\n",
    "    author2idx = global_idx[\"author2idx\"]\n",
    "    topic2idx = global_idx[\"topic2idx\"]\n",
    "\n",
    "    speech_ids_window = get_speeches_in_window(d, lookback_days, speeches_by_date)\n",
    "    \n",
    "    \n",
    "    print(len(speech_ids_window))\n",
    "    \n",
    "    # --- Build local indices ---\n",
    "    local_speech_ids = sorted(set(speech_ids_window))\n",
    "    speech_i2sid= {i: sid for i, sid in enumerate(local_speech_ids)}\n",
    "    speech_sid2i = {sid: i for i, sid in enumerate(local_speech_ids)}\n",
    "\n",
    "    # --- Local authors ---\n",
    "    author_names_window = sorted({speeches[sid][\"author\"] for sid in local_speech_ids})\n",
    "    author_i2name = {i: name for i, name in enumerate(author_names_window)}\n",
    "    author_name2i = {name: i for i, name in author_i2name.items()}\n",
    "    \n",
    "    topic_names_window = set()\n",
    "    for sid in local_speech_ids:\n",
    "        topic_names_window |= set(topic_scores[sid].keys())\n",
    "\n",
    "    topic_names_window = sorted(topic_names_window)\n",
    "    topic_i2name = {i: name for i, name in enumerate(topic_names_window)}\n",
    "    topic_name2i = {name: i for i, name in topic_i2name.items()}\n",
    "    \n",
    "    num_authors = len(author_i2name)\n",
    "    num_speeches = len(speech_i2sid)\n",
    "    num_topics = len(topic_i2name)\n",
    "\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # --- AUTHOR NODES ---\n",
    "    author_feats = []\n",
    "    for i in range(num_authors):\n",
    "        name = author_i2name[i]\n",
    "        author_feats.append([author2idx.get(name, -1)])\n",
    "    data[\"author\"].x = torch.tensor(author_feats, dtype=torch.float32)\n",
    "   \n",
    "    # --- SPEECH NODES: ACTUAL EMBEDDINGS ---\n",
    "    speech_features = []\n",
    "    for i in range(num_speeches):\n",
    "        \n",
    "        sid = speech_i2sid[i]\n",
    "\n",
    "        if \"embedding\" not in speeches_with_embeddings[sid]:\n",
    "            raise ValueError(f\"Speech {sid} has no embedding: add speeches[sid]['embedding']\")\n",
    "\n",
    "        emb = speeches_with_embeddings[sid][\"embedding\"]\n",
    "        speech_features.append(emb)\n",
    "\n",
    "    data[\"speech\"].x = torch.tensor(speech_features, dtype=torch.float32)\n",
    "   \n",
    "    # ================================\n",
    "    # 3) TOPIC NODES\n",
    "    # ================================\n",
    "    topic_feats = []\n",
    "    for i in range(num_topics):\n",
    "        name = topic_i2name[i]\n",
    "        topic_feats.append([topic2idx.get(name, -1)])\n",
    "    data[\"topic\"].x = torch.tensor(topic_feats, dtype=torch.float32)\n",
    "\n",
    "    # ================================\n",
    "    # 4) DAY NODE\n",
    "    # ================================\n",
    "    # today_rate = rates_df.loc[d, target_column]\n",
    "    today_idx = date2idx[d]\n",
    "    data[\"day\"].x = torch.tensor([[today_idx]], dtype=torch.float32)\n",
    "   \n",
    "     # ---- AUTHOR -> SPEECH ----\n",
    "    author_src, speech_dst = [], []\n",
    "    for sid in local_speech_ids:\n",
    "        a_name = speeches[sid][\"author\"]\n",
    "        a_i = author_name2i[a_name]\n",
    "        s_i = speech_sid2i[sid]\n",
    "\n",
    "        author_src.append(a_i)\n",
    "        speech_dst.append(s_i)\n",
    "\n",
    "    data[\"author\", \"gives\", \"speech\"].edge_index = torch.tensor(\n",
    "        [author_src, speech_dst], dtype=torch.long\n",
    "    )\n",
    "  \n",
    "    # ---- SPEECH -> TOPIC (edge_attr = topic score) ----\n",
    "    \n",
    "    st_src, st_dst, st_attr = [], [], []\n",
    "    \n",
    "    for sid in local_speech_ids:\n",
    "        \n",
    "        s_i = speech_sid2i[sid]\n",
    "\n",
    "        for tname, score in topic_scores[sid].items():\n",
    "            t_i = topic_name2i[tname]\n",
    "\n",
    "            st_src.append(s_i)\n",
    "            st_dst.append(t_i)\n",
    "            st_attr.append([float(score)])  # score â†’ edge feature\n",
    "    \n",
    "    data[\"speech\", \"mentions\", \"topic\"].edge_index = torch.tensor(\n",
    "            [st_src, st_dst], dtype=torch.long\n",
    "        )\n",
    "    data[\"speech\", \"mentions\", \"topic\"].edge_attr = torch.tensor(\n",
    "            st_attr, dtype=torch.float32\n",
    "        )\n",
    "  \n",
    "    # ---- DAY -> SPEECH (recency edges) ---- \n",
    "    day_src, day_dst, day_attr = [], [], []\n",
    "    for sid in local_speech_ids:\n",
    "        \n",
    "        s_i = speech_sid2i[sid]\n",
    "        sdate = speeches[sid][\"date\"]\n",
    "        lag = (d - sdate).days\n",
    "        decay = np.exp(-lag / 10.0)\n",
    "\n",
    "        day_src.append(0)  # only one day node\n",
    "        day_dst.append(s_i)\n",
    "        day_attr.append([lag, decay])\n",
    "\n",
    "    # reverse edge \n",
    "    # ????\n",
    "    data[\"day\", \"references\", \"speech\"].edge_index = torch.tensor(\n",
    "        [day_src, day_dst], dtype=torch.long\n",
    "    )\n",
    "    data[\"day\", \"references\", \"speech\"].edge_attr = torch.tensor(day_attr, dtype=torch.float32)\n",
    "    \n",
    "    data[\"speech\", \"rev_gives\", \"author\"].edge_index = torch.tensor(\n",
    "        [speech_dst, author_src], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    data[\"topic\", \"rev_mentions\", \"speech\"].edge_index = torch.tensor(\n",
    "        [st_dst, st_src], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    data[\"speech\", \"rev_references\", \"day\"].edge_index = torch.tensor(\n",
    "        [day_dst, day_src], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # author - topic edge is missing now \n",
    "    print(num_authors, num_speeches, num_topics)\n",
    "    \n",
    "    # ================================\n",
    "    # 7) TARGET for prediction\n",
    "    # ================================\n",
    "    y = float(rates_df.loc[d, target_column])\n",
    "    data.y = torch.tensor([y], dtype=torch.float32)\n",
    "\n",
    "    data.date = torch.tensor([today_idx], dtype=torch.long)\n",
    "    return data"
   ],
   "id": "3409255c1a0030b7",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:53:08.232749Z",
     "start_time": "2025-12-10T05:53:07.505726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "speeches = analysis_utils.load_speeches()\n",
    "topic_scores = analysis_utils.load_topic_scores_by_sid()\n",
    "rates_df = analysis_utils.load_rates()\n",
    "speeches_with_embeddings = analysis_utils.load_speeches_with_embeddings()\n",
    "\n",
    "graphs = build_all_graphs(\n",
    "        speeches,\n",
    "        speeches_with_embeddings,\n",
    "        topic_scores,\n",
    "        rates_df,\n",
    "        out_dir=\"graphs_ffr_delta\",   # change as you like\n",
    "        lookback_days=LOOKBACK_DAYS,\n",
    "        target_column=TARGET_COLUMN,\n",
    "    )\n"
   ],
   "id": "8a720c2c6f35a8e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977 647\n",
      "647\n",
      "1\n",
      "1 1 6\n",
      "2\n",
      "2 2 6\n",
      "3\n",
      "3 3 6\n",
      "5\n",
      "5 5 6\n",
      "5\n",
      "4 5 6\n",
      "5\n",
      "3 5 6\n",
      "3\n",
      "2 3 6\n",
      "2\n",
      "2 2 6\n",
      "3\n",
      "3 3 6\n",
      "3\n",
      "3 3 6\n",
      "4\n",
      "3 4 6\n",
      "5\n",
      "4 5 6\n",
      "6\n",
      "4 6 6\n",
      "5\n",
      "3 5 6\n",
      "6\n",
      "4 6 6\n",
      "8\n",
      "4 8 6\n",
      "9\n",
      "4 9 6\n",
      "12\n",
      "6 12 6\n",
      "13\n",
      "7 13 6\n",
      "13\n",
      "7 13 6\n",
      "12\n",
      "7 12 6\n",
      "13\n",
      "7 13 6\n",
      "14\n",
      "7 14 6\n",
      "15\n",
      "8 15 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "10\n",
      "8 10 6\n",
      "11\n",
      "9 11 6\n",
      "12\n",
      "9 12 6\n",
      "11\n",
      "9 11 6\n",
      "7\n",
      "6 7 6\n",
      "8\n",
      "7 8 6\n",
      "9\n",
      "8 9 6\n",
      "10\n",
      "8 10 6\n",
      "12\n",
      "8 12 6\n",
      "13\n",
      "8 13 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "8 15 6\n",
      "2\n",
      "2 2 6\n",
      "3\n",
      "3 3 6\n",
      "5\n",
      "5 5 6\n",
      "6\n",
      "6 6 6\n",
      "7\n",
      "7 7 6\n",
      "9\n",
      "8 9 6\n",
      "11\n",
      "9 11 6\n",
      "9\n",
      "7 9 6\n",
      "10\n",
      "8 10 6\n",
      "12\n",
      "8 12 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "8 15 6\n",
      "14\n",
      "7 14 6\n",
      "18\n",
      "9 18 6\n",
      "19\n",
      "9 19 6\n",
      "23\n",
      "9 23 6\n",
      "25\n",
      "10 25 6\n",
      "25\n",
      "10 25 6\n",
      "24\n",
      "10 24 6\n",
      "23\n",
      "10 23 6\n",
      "24\n",
      "10 24 6\n",
      "24\n",
      "10 24 6\n",
      "14\n",
      "9 14 6\n",
      "16\n",
      "10 16 6\n",
      "19\n",
      "11 19 6\n",
      "20\n",
      "11 20 6\n",
      "18\n",
      "10 18 6\n",
      "17\n",
      "10 17 6\n",
      "15\n",
      "9 15 6\n",
      "15\n",
      "8 15 6\n",
      "17\n",
      "8 17 6\n",
      "18\n",
      "8 18 6\n",
      "20\n",
      "9 20 6\n",
      "13\n",
      "9 13 6\n",
      "12\n",
      "9 12 6\n",
      "14\n",
      "10 14 6\n",
      "15\n",
      "11 15 6\n",
      "15\n",
      "12 15 6\n",
      "16\n",
      "11 16 6\n",
      "14\n",
      "10 14 6\n",
      "15\n",
      "10 15 6\n",
      "16\n",
      "10 16 6\n",
      "15\n",
      "8 15 6\n",
      "18\n",
      "8 18 6\n",
      "19\n",
      "9 19 6\n",
      "20\n",
      "9 20 6\n",
      "22\n",
      "10 22 6\n",
      "23\n",
      "10 23 6\n",
      "22\n",
      "9 22 6\n",
      "24\n",
      "9 24 6\n",
      "24\n",
      "9 24 6\n",
      "23\n",
      "8 23 6\n",
      "17\n",
      "7 17 6\n",
      "12\n",
      "8 12 6\n",
      "13\n",
      "9 13 6\n",
      "16\n",
      "10 16 6\n",
      "17\n",
      "10 17 6\n",
      "15\n",
      "10 15 6\n",
      "16\n",
      "11 16 6\n",
      "12\n",
      "10 12 6\n",
      "13\n",
      "10 13 6\n",
      "13\n",
      "9 13 6\n",
      "16\n",
      "10 16 6\n",
      "17\n",
      "10 17 6\n",
      "18\n",
      "10 18 6\n",
      "10\n",
      "7 10 6\n",
      "12\n",
      "8 12 6\n",
      "4\n",
      "3 4 6\n",
      "5\n",
      "4 5 6\n",
      "6\n",
      "5 6 6\n",
      "7\n",
      "6 7 6\n",
      "9\n",
      "7 9 6\n",
      "8\n",
      "6 8 6\n",
      "8\n",
      "6 8 6\n",
      "8\n",
      "5 8 6\n",
      "10\n",
      "6 10 6\n",
      "12\n",
      "7 12 6\n",
      "13\n",
      "6 13 6\n",
      "14\n",
      "6 14 6\n",
      "14\n",
      "6 14 6\n",
      "13\n",
      "7 13 6\n",
      "13\n",
      "7 13 6\n",
      "14\n",
      "7 14 6\n",
      "15\n",
      "7 15 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "20\n",
      "10 20 6\n",
      "21\n",
      "11 21 6\n",
      "23\n",
      "11 23 6\n",
      "24\n",
      "11 24 6\n",
      "24\n",
      "11 24 6\n",
      "18\n",
      "10 18 6\n",
      "16\n",
      "11 16 6\n",
      "17\n",
      "12 17 6\n",
      "17\n",
      "11 17 6\n",
      "15\n",
      "9 15 6\n",
      "16\n",
      "9 16 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "14\n",
      "9 14 6\n",
      "15\n",
      "10 15 6\n",
      "16\n",
      "10 16 6\n",
      "4\n",
      "4 4 6\n",
      "5\n",
      "4 5 6\n",
      "3\n",
      "2 3 6\n",
      "4\n",
      "3 4 6\n",
      "5\n",
      "3 5 6\n",
      "6\n",
      "3 6 6\n",
      "7\n",
      "4 7 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "9\n",
      "6 9 6\n",
      "7\n",
      "5 7 6\n",
      "6\n",
      "5 6 6\n",
      "9\n",
      "7 9 6\n",
      "10\n",
      "7 10 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "7 12 6\n",
      "13\n",
      "7 13 6\n",
      "14\n",
      "8 14 6\n",
      "17\n",
      "9 17 6\n",
      "14\n",
      "7 14 6\n",
      "10\n",
      "6 10 6\n",
      "8\n",
      "4 8 6\n",
      "4\n",
      "4 4 6\n",
      "5\n",
      "5 5 6\n",
      "5\n",
      "5 5 6\n",
      "4\n",
      "4 4 6\n",
      "5\n",
      "5 5 6\n",
      "6\n",
      "5 6 6\n",
      "7\n",
      "6 7 6\n",
      "6\n",
      "5 6 6\n",
      "10\n",
      "7 10 6\n",
      "11\n",
      "7 11 6\n",
      "7\n",
      "7 7 6\n",
      "8\n",
      "7 8 6\n",
      "9\n",
      "8 9 6\n",
      "9\n",
      "7 9 6\n",
      "12\n",
      "9 12 6\n",
      "9\n",
      "8 9 6\n",
      "10\n",
      "8 10 6\n",
      "11\n",
      "8 11 6\n",
      "13\n",
      "10 13 6\n",
      "14\n",
      "11 14 6\n",
      "14\n",
      "10 14 6\n",
      "8\n",
      "6 8 6\n",
      "8\n",
      "5 8 6\n",
      "9\n",
      "6 9 6\n",
      "9\n",
      "6 9 6\n",
      "8\n",
      "6 8 6\n",
      "6\n",
      "4 6 6\n",
      "8\n",
      "5 8 6\n",
      "9\n",
      "5 9 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "7 12 6\n",
      "14\n",
      "8 14 6\n",
      "11\n",
      "9 11 6\n",
      "12\n",
      "9 12 6\n",
      "9\n",
      "6 9 6\n",
      "12\n",
      "8 12 6\n",
      "12\n",
      "8 12 6\n",
      "13\n",
      "8 13 6\n",
      "12\n",
      "8 12 6\n",
      "14\n",
      "9 14 6\n",
      "15\n",
      "9 15 6\n",
      "16\n",
      "9 16 6\n",
      "17\n",
      "10 17 6\n",
      "20\n",
      "11 20 6\n",
      "22\n",
      "11 22 6\n",
      "23\n",
      "11 23 6\n",
      "24\n",
      "11 24 6\n",
      "22\n",
      "12 22 6\n",
      "10\n",
      "7 10 6\n",
      "13\n",
      "8 13 6\n",
      "8\n",
      "6 8 6\n",
      "9\n",
      "6 9 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "7 10 6\n",
      "11\n",
      "7 11 6\n",
      "8\n",
      "6 8 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "5\n",
      "4 5 6\n",
      "6\n",
      "5 6 6\n",
      "7\n",
      "6 7 6\n",
      "9\n",
      "6 9 6\n",
      "8\n",
      "5 8 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "7\n",
      "6 7 6\n",
      "5\n",
      "5 5 6\n",
      "6\n",
      "6 6 6\n",
      "7\n",
      "7 7 6\n",
      "8\n",
      "7 8 6\n",
      "10\n",
      "8 10 6\n",
      "12\n",
      "10 12 6\n",
      "13\n",
      "10 13 6\n",
      "15\n",
      "10 15 6\n",
      "16\n",
      "10 16 6\n",
      "12\n",
      "9 12 6\n",
      "12\n",
      "8 12 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "9 15 6\n",
      "12\n",
      "9 12 6\n",
      "13\n",
      "9 13 6\n",
      "11\n",
      "8 11 6\n",
      "11\n",
      "9 11 6\n",
      "12\n",
      "10 12 6\n",
      "13\n",
      "10 13 6\n",
      "14\n",
      "10 14 6\n",
      "17\n",
      "11 17 6\n",
      "9\n",
      "7 9 6\n",
      "14\n",
      "8 14 6\n",
      "17\n",
      "9 17 6\n",
      "19\n",
      "9 19 6\n",
      "19\n",
      "10 19 6\n",
      "16\n",
      "9 16 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "19\n",
      "10 19 6\n",
      "21\n",
      "12 21 6\n",
      "22\n",
      "12 22 6\n",
      "21\n",
      "12 21 6\n",
      "23\n",
      "12 23 6\n",
      "9\n",
      "6 9 6\n",
      "11\n",
      "8 11 6\n",
      "11\n",
      "9 11 6\n",
      "11\n",
      "10 11 6\n",
      "11\n",
      "8 11 6\n",
      "11\n",
      "7 11 6\n",
      "9\n",
      "7 9 6\n",
      "10\n",
      "7 10 6\n",
      "4\n",
      "4 4 6\n",
      "2\n",
      "2 2 6\n",
      "3\n",
      "3 3 6\n",
      "5\n",
      "4 5 6\n",
      "5\n",
      "4 5 6\n",
      "6\n",
      "5 6 6\n",
      "9\n",
      "7 9 6\n",
      "9\n",
      "6 9 6\n",
      "9\n",
      "5 9 6\n",
      "9\n",
      "5 9 6\n",
      "10\n",
      "5 10 6\n",
      "12\n",
      "6 12 6\n",
      "12\n",
      "6 12 6\n",
      "14\n",
      "7 14 6\n",
      "15\n",
      "7 15 6\n",
      "11\n",
      "8 11 6\n",
      "13\n",
      "8 13 6\n",
      "14\n",
      "9 14 6\n",
      "15\n",
      "9 15 6\n",
      "16\n",
      "10 16 6\n",
      "17\n",
      "10 17 6\n",
      "18\n",
      "10 18 6\n",
      "11\n",
      "7 11 6\n",
      "9\n",
      "7 9 6\n",
      "10\n",
      "8 10 6\n",
      "10\n",
      "7 10 6\n",
      "11\n",
      "7 11 6\n",
      "13\n",
      "7 13 6\n",
      "15\n",
      "9 15 6\n",
      "9\n",
      "7 9 6\n",
      "2\n",
      "2 2 6\n",
      "4\n",
      "4 4 6\n",
      "3\n",
      "3 3 6\n",
      "2\n",
      "1 2 6\n",
      "4\n",
      "3 4 6\n",
      "5\n",
      "4 5 6\n",
      "6\n",
      "5 6 6\n",
      "8\n",
      "6 8 6\n",
      "9\n",
      "7 9 6\n",
      "9\n",
      "8 9 6\n",
      "7\n",
      "7 7 6\n",
      "8\n",
      "7 8 6\n",
      "7\n",
      "6 7 6\n",
      "6\n",
      "6 6 6\n",
      "7\n",
      "6 7 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "6 10 6\n",
      "12\n",
      "6 12 6\n",
      "11\n",
      "5 11 6\n",
      "6\n",
      "4 6 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "5 8 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "8\n",
      "7 8 6\n",
      "9\n",
      "8 9 6\n",
      "10\n",
      "8 10 6\n",
      "11\n",
      "8 11 6\n",
      "13\n",
      "9 13 6\n",
      "14\n",
      "9 14 6\n",
      "8\n",
      "6 8 6\n",
      "9\n",
      "6 9 6\n",
      "8\n",
      "6 8 6\n",
      "8\n",
      "6 8 6\n",
      "6\n",
      "6 6 6\n",
      "7\n",
      "6 7 6\n",
      "6\n",
      "5 6 6\n",
      "5\n",
      "4 5 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "3 8 6\n",
      "10\n",
      "3 10 6\n",
      "11\n",
      "4 11 6\n",
      "12\n",
      "5 12 6\n",
      "12\n",
      "5 12 6\n",
      "13\n",
      "6 13 6\n",
      "8\n",
      "5 8 6\n",
      "9\n",
      "7 9 6\n",
      "10\n",
      "8 10 6\n",
      "11\n",
      "8 11 6\n",
      "12\n",
      "9 12 6\n",
      "15\n",
      "11 15 6\n",
      "16\n",
      "11 16 6\n",
      "13\n",
      "10 13 6\n",
      "14\n",
      "10 14 6\n",
      "16\n",
      "11 16 6\n",
      "17\n",
      "11 17 6\n",
      "19\n",
      "12 19 6\n",
      "20\n",
      "12 20 6\n",
      "10\n",
      "8 10 6\n",
      "9\n",
      "7 9 6\n",
      "11\n",
      "8 11 6\n",
      "9\n",
      "6 9 6\n",
      "11\n",
      "7 11 6\n",
      "13\n",
      "8 13 6\n",
      "14\n",
      "8 14 6\n",
      "12\n",
      "9 12 6\n",
      "14\n",
      "10 14 6\n",
      "15\n",
      "11 15 6\n",
      "16\n",
      "12 16 6\n",
      "9\n",
      "9 9 6\n",
      "3\n",
      "3 3 6\n",
      "4\n",
      "4 4 6\n",
      "8\n",
      "7 8 6\n",
      "11\n",
      "8 11 6\n",
      "13\n",
      "9 13 6\n",
      "15\n",
      "9 15 6\n",
      "16\n",
      "7 16 6\n",
      "18\n",
      "8 18 6\n",
      "21\n",
      "8 21 6\n",
      "23\n",
      "8 23 6\n",
      "24\n",
      "9 24 6\n",
      "19\n",
      "9 19 6\n",
      "21\n",
      "9 21 6\n",
      "23\n",
      "9 23 6\n",
      "26\n",
      "9 26 6\n",
      "27\n",
      "10 27 6\n",
      "21\n",
      "9 21 6\n",
      "10\n",
      "5 10 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "7 10 6\n",
      "12\n",
      "8 12 6\n",
      "7\n",
      "7 7 6\n",
      "8\n",
      "8 8 6\n",
      "9\n",
      "9 9 6\n",
      "10\n",
      "9 10 6\n",
      "11\n",
      "9 11 6\n",
      "12\n",
      "9 12 6\n",
      "17\n",
      "10 17 6\n",
      "12\n",
      "7 12 6\n",
      "12\n",
      "7 12 6\n",
      "14\n",
      "8 14 6\n",
      "13\n",
      "8 13 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "8 15 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "7 12 6\n",
      "13\n",
      "8 13 6\n",
      "16\n",
      "8 16 6\n",
      "18\n",
      "9 18 6\n",
      "13\n",
      "7 13 6\n",
      "10\n",
      "5 10 6\n",
      "10\n",
      "6 10 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "7 11 6\n",
      "7\n",
      "5 7 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "6 11 6\n",
      "11\n",
      "6 11 6\n",
      "6\n",
      "4 6 6\n",
      "7\n",
      "4 7 6\n",
      "8\n",
      "5 8 6\n",
      "7\n",
      "5 7 6\n",
      "5\n",
      "2 5 6\n",
      "7\n",
      "3 7 6\n",
      "8\n",
      "4 8 6\n",
      "9\n",
      "5 9 6\n",
      "8\n",
      "5 8 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "7 10 6\n",
      "10\n",
      "7 10 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "7 12 6\n",
      "13\n",
      "7 13 6\n",
      "14\n",
      "8 14 6\n",
      "12\n",
      "6 12 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "9 15 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "19\n",
      "10 19 6\n",
      "21\n",
      "10 21 6\n",
      "23\n",
      "10 23 6\n",
      "26\n",
      "10 26 6\n",
      "30\n",
      "10 30 6\n",
      "32\n",
      "10 32 6\n",
      "31\n",
      "10 31 6\n",
      "31\n",
      "10 31 6\n",
      "31\n",
      "10 31 6\n",
      "26\n",
      "9 26 6\n",
      "27\n",
      "9 27 6\n",
      "28\n",
      "9 28 6\n",
      "27\n",
      "10 27 6\n",
      "26\n",
      "10 26 6\n",
      "24\n",
      "9 24 6\n",
      "23\n",
      "11 23 6\n",
      "17\n",
      "10 17 6\n",
      "18\n",
      "11 18 6\n",
      "19\n",
      "11 19 6\n",
      "21\n",
      "11 21 6\n",
      "22\n",
      "11 22 6\n",
      "1\n",
      "1 1 6\n",
      "2\n",
      "2 2 6\n",
      "3\n",
      "3 3 6\n",
      "4\n",
      "4 4 6\n",
      "6\n",
      "5 6 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "10\n",
      "7 10 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "6 11 6\n",
      "12\n",
      "6 12 6\n",
      "12\n",
      "7 12 6\n",
      "15\n",
      "9 15 6\n",
      "17\n",
      "9 17 6\n",
      "19\n",
      "10 19 6\n",
      "20\n",
      "10 20 6\n",
      "23\n",
      "11 23 6\n",
      "23\n",
      "11 23 6\n",
      "23\n",
      "11 23 6\n",
      "12\n",
      "8 12 6\n",
      "6\n",
      "3 6 6\n",
      "9\n",
      "5 9 6\n",
      "9\n",
      "5 9 6\n",
      "12\n",
      "7 12 6\n",
      "12\n",
      "8 12 6\n",
      "13\n",
      "9 13 6\n",
      "14\n",
      "9 14 6\n",
      "9\n",
      "6 9 6\n",
      "6\n",
      "4 6 6\n",
      "8\n",
      "5 8 6\n",
      "7\n",
      "4 7 6\n",
      "8\n",
      "4 8 6\n",
      "9\n",
      "6 9 6\n",
      "12\n",
      "8 12 6\n",
      "12\n",
      "9 12 6\n",
      "13\n",
      "10 13 6\n",
      "16\n",
      "10 16 6\n",
      "17\n",
      "10 17 6\n",
      "18\n",
      "10 18 6\n",
      "20\n",
      "10 20 6\n",
      "21\n",
      "11 21 6\n",
      "11\n",
      "8 11 6\n",
      "12\n",
      "9 12 6\n",
      "9\n",
      "8 9 6\n",
      "11\n",
      "8 11 6\n",
      "12\n",
      "8 12 6\n",
      "11\n",
      "7 11 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "7 12 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "8 15 6\n",
      "16\n",
      "8 16 6\n",
      "15\n",
      "7 15 6\n",
      "15\n",
      "7 15 6\n",
      "15\n",
      "6 15 6\n",
      "7\n",
      "4 7 6\n",
      "4\n",
      "3 4 6\n",
      "5\n",
      "3 5 6\n",
      "4\n",
      "3 4 6\n",
      "5\n",
      "3 5 6\n",
      "7\n",
      "4 7 6\n",
      "8\n",
      "5 8 6\n",
      "9\n",
      "6 9 6\n",
      "9\n",
      "7 9 6\n",
      "9\n",
      "6 9 6\n",
      "10\n",
      "7 10 6\n",
      "14\n",
      "8 14 6\n",
      "15\n",
      "8 15 6\n",
      "16\n",
      "8 16 6\n",
      "17\n",
      "8 17 6\n",
      "17\n",
      "8 17 6\n",
      "19\n",
      "8 19 6\n",
      "18\n",
      "8 18 6\n",
      "19\n",
      "8 19 6\n",
      "20\n",
      "9 20 6\n",
      "21\n",
      "10 21 6\n",
      "21\n",
      "10 21 6\n",
      "20\n",
      "9 20 6\n",
      "21\n",
      "10 21 6\n",
      "22\n",
      "10 22 6\n",
      "14\n",
      "8 14 6\n",
      "8\n",
      "5 8 6\n",
      "8\n",
      "5 8 6\n",
      "10\n",
      "7 10 6\n",
      "11\n",
      "7 11 6\n",
      "11\n",
      "8 11 6\n",
      "10\n",
      "7 10 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "7 12 6\n",
      "1\n",
      "1 1 6\n",
      "4\n",
      "4 4 6\n",
      "5\n",
      "5 5 6\n",
      "6\n",
      "5 6 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "6 8 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "7 11 6\n",
      "11\n",
      "7 11 6\n",
      "9\n",
      "5 9 6\n",
      "10\n",
      "6 10 6\n",
      "11\n",
      "7 11 6\n",
      "13\n",
      "9 13 6\n",
      "14\n",
      "9 14 6\n",
      "16\n",
      "9 16 6\n",
      "17\n",
      "9 17 6\n",
      "19\n",
      "9 19 6\n",
      "22\n",
      "9 22 6\n",
      "21\n",
      "9 21 6\n",
      "23\n",
      "10 23 6\n",
      "15\n",
      "9 15 6\n",
      "13\n",
      "8 13 6\n",
      "15\n",
      "8 15 6\n",
      "11\n",
      "7 11 6\n",
      "12\n",
      "8 12 6\n",
      "14\n",
      "8 14 6\n",
      "10\n",
      "6 10 6\n",
      "13\n",
      "9 13 6\n",
      "14\n",
      "9 14 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "19\n",
      "9 19 6\n",
      "18\n",
      "9 18 6\n",
      "18\n",
      "9 18 6\n",
      "15\n",
      "8 15 6\n",
      "12\n",
      "6 12 6\n",
      "11\n",
      "6 11 6\n",
      "13\n",
      "6 13 6\n",
      "13\n",
      "7 13 6\n",
      "13\n",
      "7 13 6\n",
      "15\n",
      "8 15 6\n",
      "16\n",
      "8 16 6\n",
      "16\n",
      "8 16 6\n",
      "16\n",
      "8 16 6\n",
      "18\n",
      "8 18 6\n",
      "19\n",
      "8 19 6\n",
      "20\n",
      "8 20 6\n",
      "21\n",
      "8 21 6\n",
      "22\n",
      "8 22 6\n",
      "23\n",
      "9 23 6\n",
      "9\n",
      "6 9 6\n",
      "12\n",
      "6 12 6\n",
      "14\n",
      "9 14 6\n",
      "15\n",
      "9 15 6\n",
      "9\n",
      "7 9 6\n",
      "13\n",
      "8 13 6\n",
      "15\n",
      "8 15 6\n",
      "17\n",
      "8 17 6\n",
      "17\n",
      "8 17 6\n",
      "10\n",
      "8 10 6\n",
      "3\n",
      "2 3 6\n",
      "4\n",
      "3 4 6\n",
      "4\n",
      "3 4 6\n",
      "6\n",
      "5 6 6\n",
      "7\n",
      "5 7 6\n",
      "8\n",
      "5 8 6\n",
      "5\n",
      "4 5 6\n",
      "7\n",
      "6 7 6\n",
      "6\n",
      "6 6 6\n",
      "9\n",
      "8 9 6\n",
      "10\n",
      "8 10 6\n",
      "10\n",
      "8 10 6\n",
      "13\n",
      "10 13 6\n",
      "15\n",
      "10 15 6\n",
      "15\n",
      "10 15 6\n",
      "17\n",
      "10 17 6\n",
      "21\n",
      "10 21 6\n",
      "23\n",
      "10 23 6\n",
      "24\n",
      "10 24 6\n",
      "27\n",
      "10 27 6\n",
      "28\n",
      "10 28 6\n",
      "29\n",
      "9 29 6\n",
      "28\n",
      "9 28 6\n",
      "21\n",
      "8 21 6\n",
      "22\n",
      "8 22 6\n",
      "20\n",
      "9 20 6\n",
      "17\n",
      "9 17 6\n",
      "18\n",
      "9 18 6\n",
      "14\n",
      "7 14 6\n",
      "15\n",
      "7 15 6\n",
      "16\n",
      "7 16 6\n",
      "17\n",
      "7 17 6\n",
      "Built 647 graphs and saved to graphs_ffr_delta\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:54:43.559885Z",
     "start_time": "2025-12-10T05:54:43.555377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "SPEECH_EMB_DIM = 768\n",
    "class SpeechHeteroGNN(nn.Module):\n",
    "    def __init__(self, num_authors, num_topics, num_days, hidden_dim=64, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # --- MLP for speech embeddings (this part is fine) ---\n",
    "        self.speech_mlp = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # --- CORRECT: Embedding layers for categorical nodes ---\n",
    "        # Note: We use embedding dimension = hidden_dim to match HGT input requirements\n",
    "        self.author_emb = nn.Embedding(num_authors + 1, hidden_dim)\n",
    "        self.topic_emb = nn.Embedding(num_topics + 1, hidden_dim)\n",
    "        self.day_emb = nn.Embedding(num_days + 1, hidden_dim)\n",
    "        \n",
    "        # --- HGTConv ---\n",
    "        self.hgt = HGTConv(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            metadata=(\n",
    "                [\"author\", \"speech\", \"topic\", \"day\"],\n",
    "                [\n",
    "                    (\"author\", \"gives\", \"speech\"),\n",
    "                    (\"speech\", \"rev_gives\", \"author\"),\n",
    "                    (\"speech\", \"mentions\", \"topic\"),\n",
    "                    (\"topic\", \"rev_mentions\", \"speech\"),\n",
    "                    (\"day\", \"references\", \"speech\"),\n",
    "                    (\"speech\", \"rev_references\", \"day\"),\n",
    "                ]\n",
    "            ),\n",
    "            heads=num_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        out_dict = {}\n",
    "\n",
    "        # Process each node type appropriately\n",
    "        for ntype, x in x_dict.items():\n",
    "            if ntype == \"speech\":\n",
    "                # Speech features are already floats (embeddings), use MLP\n",
    "                out_dict[ntype] = self.speech_mlp(x.float())\n",
    "            elif ntype == \"author\":\n",
    "                # x is [num_nodes, 1], we need [num_nodes] for Embedding\n",
    "                out_dict[ntype] = self.author_emb(x.long().squeeze())\n",
    "            elif ntype == \"topic\":\n",
    "                out_dict[ntype] = self.topic_emb(x.long().squeeze())\n",
    "            elif ntype == \"day\":\n",
    "                out_dict[ntype] = self.day_emb(x.long().squeeze())\n",
    "\n",
    "        out_dict = self.hgt(out_dict, edge_index_dict)\n",
    "        return out_dict\n"
   ],
   "id": "28b0aa3bfcbdab82",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:53:17.875384Z",
     "start_time": "2025-12-10T05:53:17.872230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TemporalPredictor(nn.Module):\n",
    "    def __init__(self, embed_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, seq_embeddings):\n",
    "        out, _ = self.gru(seq_embeddings)\n",
    "        return self.fc(out[:, -1])  # last timestep\n"
   ],
   "id": "f6d9c79e490ec043",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T05:53:21.311987Z",
     "start_time": "2025-12-10T05:53:21.308345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FedSpeechModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gnn = SpeechHeteroGNN(hidden_dim)\n",
    "        self.temporal = TemporalPredictor(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, graph_seq):\n",
    "        day_embs = []\n",
    "        \n",
    "        for g in graph_seq:\n",
    "            x_dict = self.gnn(g.x_dict, g.edge_index_dict)\n",
    "            z = x_dict[\"day\"]  # shape [1, hidden_dim]\n",
    "            day_embs.append(z)\n",
    "        \n",
    "        day_embs = torch.stack(day_embs, dim=1)  # [1, seq_len, hidden_dim]\n",
    "        return self.temporal(day_embs)\n"
   ],
   "id": "1e8199268912fdb7",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:01:19.474401Z",
     "start_time": "2025-12-10T05:54:49.688612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "\n",
    "model = FedSpeechModel(hidden_dim=64)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "L = 30        # sequence length\n",
    "EPOCHS = 40   # number of epochs\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for t in range(L, len(graphs)):\n",
    "        seq = graphs[t-L:t]\n",
    "        target = graphs[t].y.float()\n",
    "\n",
    "        pred = model(seq)\n",
    "        loss = loss_fn(pred.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS}  MSE={total_loss/count:.6f}\")\n"
   ],
   "id": "3f83fd73bf447885",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingshu/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:634: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40  MSE=0.004394\n",
      "Epoch 2/40  MSE=0.003549\n",
      "Epoch 3/40  MSE=0.003473\n",
      "Epoch 4/40  MSE=0.003502\n",
      "Epoch 5/40  MSE=0.003511\n",
      "Epoch 6/40  MSE=0.003675\n",
      "Epoch 7/40  MSE=0.003464\n",
      "Epoch 8/40  MSE=0.003463\n",
      "Epoch 9/40  MSE=0.003508\n",
      "Epoch 10/40  MSE=0.003467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[95]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     21\u001B[39m loss = loss_fn(pred.squeeze(), target)\n\u001B[32m     23\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m optimizer.step()\n\u001B[32m     27\u001B[39m total_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:20:01.938599Z",
     "start_time": "2025-12-10T06:20:01.935253Z"
    }
   },
   "cell_type": "code",
   "source": "print(0.003464**0.5)",
   "id": "aa3c11c2744fc976",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05885575587824864\n"
     ]
    }
   ],
   "execution_count": 96
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
