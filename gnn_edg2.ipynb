{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T06:28:28.447763Z",
     "start_time": "2025-12-11T06:28:27.830995Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HGTConv\n",
    "from torch_geometric.data import HeteroData\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ==========================================\n",
    "# 1. CORRECTED GNN MODEL\n",
    "# ==========================================\n",
    "class SpeechHeteroGNN(nn.Module):\n",
    "    def __init__(self, global_idx, hidden_dim=64, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # --- A. Embeddings for ID-based nodes ---\n",
    "        # We add +1 to max index to be safe, or len(idx) if indices are 0..N-1\n",
    "        num_authors = len(global_idx[\"author2idx\"]) + 1\n",
    "        num_topics = len(global_idx[\"topic2idx\"]) + 1\n",
    "        num_days = len(global_idx[\"date2idx\"]) + 1\n",
    "\n",
    "        self.emb_dict = nn.ModuleDict({\n",
    "            \"author\": nn.Embedding(num_authors, hidden_dim),\n",
    "            \"topic\":  nn.Embedding(num_topics, hidden_dim),\n",
    "            \"day\":    nn.Embedding(num_days, hidden_dim),\n",
    "        })\n",
    "\n",
    "        # --- B. Projection for Speech Nodes ---\n",
    "        # Input: 768 (BERT) + 1 (Topic Score) + 2 (Lag, Decay) = 771\n",
    "        self.speech_lin = nn.Linear(768 + 3, hidden_dim)\n",
    "\n",
    "        # --- C. HGT Convolution ---\n",
    "        # Metadata must match the edge types in the graph exactly\n",
    "        self.hgt = HGTConv(\n",
    "            in_channels=hidden_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            metadata=(\n",
    "                [\"author\", \"speech\", \"topic\", \"day\"],\n",
    "                [\n",
    "                    (\"author\", \"gives\", \"speech\"),\n",
    "                    (\"speech\", \"rev_gives\", \"author\"),\n",
    "                    (\"speech\", \"mentions\", \"topic\"),\n",
    "                    (\"topic\", \"rev_mentions\", \"speech\"),\n",
    "                    (\"day\", \"references\", \"speech\"),\n",
    "                    (\"speech\", \"rev_references\", \"day\"),\n",
    "                ]\n",
    "            ),\n",
    "            heads=num_heads\n",
    "        )\n",
    "\n",
    "    def compute_edge_aggregates(self, data, device):\n",
    "        \"\"\"\n",
    "        Manually aggregates edge attributes into speech nodes because \n",
    "        standard HGTConv does not use edge_attr.\n",
    "        \"\"\"\n",
    "        num_speech = data[\"speech\"].x.size(0)\n",
    "        # [mean_topic_score, mean_lag, mean_decay]\n",
    "        extras = torch.zeros(num_speech, 3, device=device)\n",
    "        counts = torch.zeros(num_speech, 3, device=device)\n",
    "\n",
    "        # 1. Topic Scores (speech -> mentions -> topic)\n",
    "        if (\"speech\", \"mentions\", \"topic\") in data.edge_types:\n",
    "            store = data[\"speech\", \"mentions\", \"topic\"]\n",
    "            if hasattr(store, \"edge_attr\") and store.edge_attr is not None:\n",
    "                # edge_attr is [score]\n",
    "                src_idx = store.edge_index[0]\n",
    "                scores = store.edge_attr.view(-1).to(device)\n",
    "                \n",
    "                # Accumulate\n",
    "                extras[:, 0].index_add_(0, src_idx, scores)\n",
    "                counts[:, 0].index_add_(0, src_idx, torch.ones_like(scores))\n",
    "\n",
    "        # 2. Time Lags (day -> references -> speech)\n",
    "        if (\"day\", \"references\", \"speech\") in data.edge_types:\n",
    "            store = data[\"day\", \"references\", \"speech\"]\n",
    "            if hasattr(store, \"edge_attr\") and store.edge_attr is not None:\n",
    "                # edge_attr is [lag, decay]\n",
    "                dst_idx = store.edge_index[1] # speech is destination here\n",
    "                attrs = store.edge_attr.to(device)\n",
    "                \n",
    "                # Accumulate\n",
    "                extras[:, 1:].index_add_(0, dst_idx, attrs)\n",
    "                counts[:, 1:].index_add_(0, dst_idx, torch.ones_like(attrs))\n",
    "\n",
    "        # Average\n",
    "        counts = torch.clamp(counts, min=1.0)\n",
    "        return extras / counts\n",
    "\n",
    "    def forward(self, data):\n",
    "        x_dict = data.x_dict\n",
    "        edge_index_dict = data.edge_index_dict\n",
    "        device = data[\"speech\"].x.device\n",
    "        \n",
    "        # --- Step 1: Embeddings for ID nodes ---\n",
    "        out_dict = {}\n",
    "        for ntype in [\"author\", \"topic\", \"day\"]:\n",
    "            # Flatten/squeeze input indices: [N, 1] -> [N]\n",
    "            idx_tensor = x_dict[ntype].long().view(-1)\n",
    "            out_dict[ntype] = self.emb_dict[ntype](idx_tensor)\n",
    "\n",
    "        # --- Step 2: Speech Features (Emb + Edges) ---\n",
    "        speech_base = x_dict[\"speech\"] # [N, 768]\n",
    "        edge_stats = self.compute_edge_aggregates(data, device) # [N, 3]\n",
    "        \n",
    "        # Concatenate and Project\n",
    "        speech_full = torch.cat([speech_base, edge_stats], dim=-1)\n",
    "        out_dict[\"speech\"] = self.speech_lin(speech_full)\n",
    "\n",
    "        # --- Step 3: Graph Convolution ---\n",
    "        out_dict = self.hgt(out_dict, edge_index_dict)\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "# ==========================================\n",
    "# 2. TEMPORAL PREDICTOR WRAPPER\n",
    "# ==========================================\n",
    "class TemporalPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, seq_embeddings):\n",
    "        # seq_embeddings: [Batch=1, Seq_Len, Hidden_Dim]\n",
    "        out, _ = self.gru(seq_embeddings)\n",
    "        return self.fc(out[:, -1])  # Predict from last step\n",
    "\n",
    "class FedSpeechModel(nn.Module):\n",
    "    def __init__(self, global_idx, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gnn = SpeechHeteroGNN(global_idx, hidden_dim)\n",
    "        self.temporal = TemporalPredictor(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, graph_seq):\n",
    "        day_embs = []\n",
    "        for g in graph_seq:\n",
    "            # Pass the whole graph object 'g' to access edge_attrs\n",
    "            x_dict = self.gnn(g)\n",
    "            \n",
    "            # We use the 'day' node embedding as the graph representation\n",
    "            z = x_dict[\"day\"] # [1, hidden_dim]\n",
    "            day_embs.append(z)\n",
    "        \n",
    "        # Stack time steps: [1, Seq_Len, Hidden_Dim]\n",
    "        day_embs = torch.stack(day_embs, dim=1)\n",
    "        return self.temporal(day_embs)\n",
    "\n",
    "# ==========================================\n",
    "# 3. CORRECTED GRAPH BUILDER FUNCTION\n",
    "# ==========================================\n",
    "def build_graph_for_date_corrected(\n",
    "    d, speeches, speeches_with_embeddings, topic_scores, rates_df,\n",
    "    speeches_by_date, global_idx, lookback_days, target_column\n",
    "):\n",
    "    \"\"\"\n",
    "    Re-implementation of your notebook function, but ensures\n",
    "    ID-based nodes (Author, Topic, Day) use torch.long for Embeddings.\n",
    "    \"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    date2idx = global_idx[\"date2idx\"]\n",
    "    author2idx = global_idx[\"author2idx\"]\n",
    "    topic2idx = global_idx[\"topic2idx\"]\n",
    "\n",
    "    # --- Helper to get window ---\n",
    "    start_date = d - timedelta(days=lookback_days - 1)\n",
    "    speech_ids_window = []\n",
    "    cur = start_date\n",
    "    while cur <= d:\n",
    "        if cur in speeches_by_date:\n",
    "            speech_ids_window.extend(speeches_by_date[cur])\n",
    "        cur += timedelta(days=1)\n",
    "    \n",
    "    # Local mappings\n",
    "    local_sids = sorted(set(speech_ids_window))\n",
    "    sid2i = {sid: i for i, sid in enumerate(local_sids)}\n",
    "    \n",
    "    # Identify active Authors/Topics\n",
    "    active_authors = sorted({speeches[sid][\"author\"] for sid in local_sids})\n",
    "    active_topics = set()\n",
    "    for sid in local_sids:\n",
    "        active_topics |= set(topic_scores[sid].keys())\n",
    "    active_topics = sorted(active_topics)\n",
    "    \n",
    "    topic_name2i = {t: i for i, t in enumerate(active_topics)}\n",
    "    author_name2i = {a: i for i, a in enumerate(active_authors)}\n",
    "\n",
    "    data = HeteroData()\n",
    "\n",
    "    # --- NODE FEATURES (CORRECTED DTYPES) ---\n",
    "    \n",
    "    # 1. Author (Long)\n",
    "    author_ids = [author2idx.get(a, 0) for a in active_authors]\n",
    "    data[\"author\"].x = torch.tensor(author_ids, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    # 2. Topic (Long)\n",
    "    topic_ids = [topic2idx.get(t, 0) for t in active_topics]\n",
    "    data[\"topic\"].x = torch.tensor(topic_ids, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "    # 3. Day (Long)\n",
    "    day_id = date2idx.get(d, 0)\n",
    "    data[\"day\"].x = torch.tensor([[day_id]], dtype=torch.long)\n",
    "\n",
    "    # 4. Speech (Float - Embeddings)\n",
    "    speech_feats = []\n",
    "    for sid in local_sids:\n",
    "        speech_feats.append(speeches_with_embeddings[sid][\"embedding\"])\n",
    "    \n",
    "    if not speech_feats: # Handle empty graph case if necessary\n",
    "        return None\n",
    "        \n",
    "    data[\"speech\"].x = torch.tensor(speech_feats, dtype=torch.float32)\n",
    "\n",
    "    # --- EDGE INDICES & ATTRIBUTES (Preserved from your notebook) ---\n",
    "    \n",
    "    # Author -> Speech\n",
    "    a_src, s_dst = [], []\n",
    "    for sid in local_sids:\n",
    "        a_name = speeches[sid][\"author\"]\n",
    "        a_src.append(author_name2i[a_name])\n",
    "        s_dst.append(sid2i[sid])\n",
    "    \n",
    "    data[\"author\", \"gives\", \"speech\"].edge_index = torch.tensor([a_src, s_dst], dtype=torch.long)\n",
    "    data[\"speech\", \"rev_gives\", \"author\"].edge_index = torch.tensor([s_dst, a_src], dtype=torch.long)\n",
    "\n",
    "    # Speech -> Topic (with Scores)\n",
    "    s_src, t_dst, scores = [], [], []\n",
    "    for sid in local_sids:\n",
    "        for tname, sc in topic_scores[sid].items():\n",
    "            s_src.append(sid2i[sid])\n",
    "            t_dst.append(topic_name2i[tname])\n",
    "            scores.append([float(sc)])\n",
    "            \n",
    "    data[\"speech\", \"mentions\", \"topic\"].edge_index = torch.tensor([s_src, t_dst], dtype=torch.long)\n",
    "    data[\"speech\", \"mentions\", \"topic\"].edge_attr = torch.tensor(scores, dtype=torch.float32)\n",
    "    # Reverse edge (no attr needed for HGT usually, but good for connectivity)\n",
    "    data[\"topic\", \"rev_mentions\", \"speech\"].edge_index = torch.tensor([t_dst, s_src], dtype=torch.long)\n",
    "\n",
    "    # Day -> Speech (with Lag/Decay)\n",
    "    d_src, s_dst_day, lags = [], [], []\n",
    "    for sid in local_sids:\n",
    "        lag = (d - speeches[sid][\"date\"]).days\n",
    "        decay = np.exp(-lag / 10.0)\n",
    "        d_src.append(0)\n",
    "        s_dst_day.append(sid2i[sid])\n",
    "        lags.append([lag, decay])\n",
    "\n",
    "    data[\"day\", \"references\", \"speech\"].edge_index = torch.tensor([d_src, s_dst_day], dtype=torch.long)\n",
    "    data[\"day\", \"references\", \"speech\"].edge_attr = torch.tensor(lags, dtype=torch.float32)\n",
    "    data[\"speech\", \"rev_references\", \"day\"].edge_index = torch.tensor([s_dst_day, d_src], dtype=torch.long)\n",
    "\n",
    "    # Target\n",
    "    y = float(rates_df.loc[d, target_column])\n",
    "    data.y = torch.tensor([y], dtype=torch.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ============================================================================\n",
    "# 0. SAFE LOADING FOR PyTorch 2.6+\n",
    "# ============================================================================\n",
    "\n",
    "torch.serialization.add_safe_globals([HeteroData])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD GRAPHS\n",
    "# ============================================================================\n",
    "\n",
    "def load_graphs(graph_dir):\n",
    "    graph_dir = Path(graph_dir)\n",
    "    graphs = []\n",
    "    for f in sorted(graph_dir.glob(\"graph_*.pt\")):\n",
    "        g = torch.load(f, weights_only=False)\n",
    "        graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION (Mock)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ASSUMPTION: You have loaded 'speeches', 'rates_df', etc. using analysis_utils\n",
    "    # and generated 'global_idx' as per your notebook.\n",
    "    \n",
    "    # 1. Build Graphs (using corrected function)\n",
    "    # graphs = [build_graph_for_date_corrected(d, ...) for d in dates]\n",
    "    \n",
    "    # 2. Initialize Model\n",
    "    # We need global_idx from your notebook\n",
    "    # mock_global_idx provided for instantiation example:\n",
    "    mock_global_idx = {\n",
    "        \"author2idx\": {\"A\":0, \"B\":1}, \n",
    "        \"topic2idx\": {\"Eco\":0}, \n",
    "        \"date2idx\": {0:0, 1:1}\n",
    "    }\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FedSpeechModel(global_idx=mock_global_idx, hidden_dim=64).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    print(\"Model initialized on:\", device)\n",
    "    \n",
    "    # 3. Training Loop\n",
    "    # Assuming 'graphs' is a list of HeteroData objects\n",
    "    # L = Sequence Length for LSTM/GRU\n",
    "    L = 30 \n",
    "    EPOCHS = 50\n",
    "    \n",
    "    # Placeholder for graphs list to prevent NameError in this script\n",
    "    #raphs = [] \n",
    "    graphs = load_graphs(\"graphs_ffr_delta\")\n",
    "\n",
    "\n",
    "    if len(graphs) > L:\n",
    "        model.train()\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            total_loss = 0.0\n",
    "            count = 0\n",
    "            \n",
    "            # Simple sliding window training\n",
    "            for t in range(L, len(graphs)):\n",
    "                # Get sequence of L graphs\n",
    "                seq_graphs = graphs[t-L : t]\n",
    "                \n",
    "                # Move to device\n",
    "                seq_graphs = [g.to(device) for g in seq_graphs]\n",
    "                target = graphs[t].y.to(device) # .view(1) if needed\n",
    "\n",
    "                # Forward\n",
    "                pred = model(seq_graphs) # Output shape [1]\n",
    "                \n",
    "                loss = loss_fn(pred.view(-1), target.view(-1))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "            \n",
    "            avg_loss = total_loss / count if count > 0 else 0\n",
    "            print(f\"Epoch {epoch:03d} | MSE Loss: {avg_loss:.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cpu\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 331\u001B[39m\n\u001B[32m    328\u001B[39m target = graphs[t].y.to(device) \u001B[38;5;66;03m# .view(1) if needed\u001B[39;00m\n\u001B[32m    330\u001B[39m \u001B[38;5;66;03m# Forward\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m331\u001B[39m pred = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseq_graphs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Output shape [1]\u001B[39;00m\n\u001B[32m    333\u001B[39m loss = loss_fn(pred.view(-\u001B[32m1\u001B[39m), target.view(-\u001B[32m1\u001B[39m))\n\u001B[32m    335\u001B[39m optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 139\u001B[39m, in \u001B[36mFedSpeechModel.forward\u001B[39m\u001B[34m(self, graph_seq)\u001B[39m\n\u001B[32m    136\u001B[39m day_embs = []\n\u001B[32m    137\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m g \u001B[38;5;129;01min\u001B[39;00m graph_seq:\n\u001B[32m    138\u001B[39m     \u001B[38;5;66;03m# Pass the whole graph object 'g' to access edge_attrs\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     x_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    141\u001B[39m     \u001B[38;5;66;03m# We use the 'day' node embedding as the graph representation\u001B[39;00m\n\u001B[32m    142\u001B[39m     z = x_dict[\u001B[33m\"\u001B[39m\u001B[33mday\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;66;03m# [1, hidden_dim]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 100\u001B[39m, in \u001B[36mSpeechHeteroGNN.forward\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m ntype \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33mauthor\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mtopic\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mday\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m     98\u001B[39m     \u001B[38;5;66;03m# Flatten/squeeze input indices: [N, 1] -> [N]\u001B[39;00m\n\u001B[32m     99\u001B[39m     idx_tensor = x_dict[ntype].long().view(-\u001B[32m1\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m     out_dict[ntype] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43memb_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mntype\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[38;5;66;03m# --- Step 2: Speech Features (Emb + Edges) ---\u001B[39;00m\n\u001B[32m    103\u001B[39m speech_base = x_dict[\u001B[33m\"\u001B[39m\u001B[33mspeech\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;66;03m# [N, 768]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:192\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/kaggle/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2542\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2536\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2537\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2538\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2539\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2540\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2541\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2542\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mIndexError\u001B[39m: index out of range in self"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
